{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this lab, you will scrape a website to get lyrics by your favorite artist. Then, you will train a Markov chain on the lyrics to try to learn that artist's \"style\". Once you have the Markov chain, you can simulate new song lyrics by that artist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Markov Chains?\n",
    "\n",
    "Markov chains are mathematical models of real-world systems that transition between multiple states. For example, the weather in San Luis Obispo from one day to the next can be modeled as a Markov chain, where the states might be \"sunny\", \"cloudy\", and \"rain\". The webpages that a user visits can also be modeled as a Markov chain, where the states are different pages on the web. This is the model underlying the PageRank algorithm that Google uses to rank its search results.\n",
    "\n",
    "Before you start this lab, please read [this visual explanation](http://setosa.io/ev/markov-chains/) to build your intuition about Markov chains. A Markov chain makes the simplifying assumption that the next state only depends on the current state and not on the past history. If we used a Markov chain to model the weather, then the probability that it will be sunny tomorrow only depends on the weather today; the weather yesterday is irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chains for Human Language\n",
    "\n",
    "We can use Markov chains to model human language. Each word is a \"state\", and the next word in a sentence only depends on the current word, not any words that came before. This model is reasonable because if the current word is \"annual\", we have some idea of what the next word might or might not be, even without knowing the previous words in the sentence:\n",
    "\n",
    "- \"report\" is a fairly likely next word\n",
    "- \"flower\" is a less likely, but still possible word\n",
    "- \"dolphins\" is a very unlikely next word\n",
    "\n",
    "The current word tells us a lot about what the next word might be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Overview\n",
    "\n",
    "- In Part 1 of this lab, you will scrape a website for lyrics.\n",
    "- In Part 2 of this lab, you will train a Markov chain that uses just the last word to predict the next word (i.e., a \"unigram\" model).\n",
    "- In Part 3 of this lab, you will train a Markov chain that uses the last two words to predict the next word (i.e., a \"bigram\" model).\n",
    "- In both Parts 2 and 3, you will generate random song lyrics using your Markov chain. \n",
    "- Finally, in Part 4, you compare how the unigram and bigram models do, in terms of generating reasonable lyrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You are permitted (but not required) to work on this lab with a partner. If you do, only one of you should submit the lab, and the other person should enter their __Github__ (NOT Cal Poly) username below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partner = \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
