{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to go for a hike in San Luis Obispo. But where? In this workbook, you will scrape [a website](http://www.hikeslo.com/) with information about local hiking routes, and build a data frame containing information about each hike (e.g., length, elevation change)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a Single Page\n",
    "\n",
    "We will use the `requests` library to fetch the contents of a URL and BeautifulSoup to parse the HTML. Although we used BeautifulSoup previously to parse XML, it's HTML where BeautifulSoup really shines. Much HTML on the web is malformed, and BeautifulSoup is designed to handle malformed HTML gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to extract the rating, location, elevation gain, and distance from the following page automatically: http://www.hikeslo.com/vaca-flats/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "req = requests.get(\"http://www.hikeslo.com/vaca-flats/\")\n",
    "soup = BeautifulSoup(req.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `soup` is a `BeautifulSoup` object that represents the document as a nested data structure. As before, to get all instances of a particular tag as a list, we can use the `.find_all()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup.find_all('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, BeautifulSoup represents the document as a nested data structure. So we can also call `.find()` and `.find_all()` on a tag to search for tags _within_ that tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table = soup.find('table')\n",
    "table.find_all('div')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use `BeautifulSoup` to extract the information that we want from the page: rating, location, elevation gain, and distance. How do we know where to find this information in the HTML source code?\n",
    "\n",
    "When web scraping, you will need to constantly go back and forth between the rendered page and the HTML source code. Google Chrome makes this easy for you. If you right click on any element on the page, one of the options is \"Inspect\". This will show you the HTML source code, with the element you selected highlighted.\n",
    "\n",
    "Let's try to extract the Location from the page automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the function below, which given a page on this site, returns the rating, location, elevation gain, and distance as a Pandas series. \n",
    "\n",
    "_Hint:_ The rating is probably the most challenging. You will need to filter based on the `class` attribute. If you have forgotten how to filter on an attribute, take a look at the documentation for `.find_all()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_for_hike(url):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return pd.Series({\n",
    "            \"rating\": None,\n",
    "            \"location\": None,\n",
    "            \"elevation_gain\": None,\n",
    "            \"distance\": None\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_data_for_hike(\"http://www.hikeslo.com/vaca-flats/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling a Site\n",
    "\n",
    "We want to be able to automatically scrape all of the hikes on the website. For example, we might want to be able to scrape information about the 10 hikes listed on [this page](http://www.hikeslo.com/).\n",
    "\n",
    "We can do this in two steps. First, we scrape the main page, getting all links to hikes. (Note: Hyperlinks are represented by the `<a>` tag.) Then, we scrape each of those pages by calling the function `get_data_for_hike()` that we wrote above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
